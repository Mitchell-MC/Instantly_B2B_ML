{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b0de761b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from time import sleep\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48c2498",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ba74a31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv(r\"C:\\Users\\Richard\\Documents\\We Cloud Data\\ad hoc\\Abu_Dhabi_Commercial_Bank.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6e6edc44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       https://www.linkedin.com/in/sameh-suleiman-asa...\n",
       "1       https://www.linkedin.com/in/danburnsmena?miniP...\n",
       "2                                                     NaN\n",
       "3       https://www.linkedin.com/in/vicky-symonds-1356...\n",
       "4       https://www.linkedin.com/in/hanine-elchayeb-ci...\n",
       "                              ...                        \n",
       "1097                                                  NaN\n",
       "1098                                                  NaN\n",
       "1099                                                  NaN\n",
       "1100                                                  NaN\n",
       "1101                                                  NaN\n",
       "Name: LinkedIn URL, Length: 1102, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['LinkedIn URL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5c3cc6fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinkedIn URL cleaning summary:\n",
      "  Original URLs (non-null): 866\n",
      "  Cleaned valid URLs: 857\n",
      "  Removed/Invalid URLs: 9\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def clean_linkedin_url(url):\n",
    "    \"\"\"\n",
    "    Clean LinkedIn URL by removing query parameters (everything after ?)\n",
    "    and handling invalid/missing URLs.\n",
    "    \n",
    "    Args:\n",
    "        url (str): Raw LinkedIn URL\n",
    "        \n",
    "    Returns:\n",
    "        str or None: Cleaned LinkedIn URL or None if invalid\n",
    "    \"\"\"\n",
    "    # Handle NaN, None, or \"N/A\" values\n",
    "    if pd.isna(url) or url == \"N/A\" or not isinstance(url, str):\n",
    "        return None\n",
    "    \n",
    "    # Remove everything after the ? (query parameters)\n",
    "    clean_url = url.split('?')[0]\n",
    "    \n",
    "    # Basic validation - check if it's a valid LinkedIn profile URL\n",
    "    linkedin_pattern = r'^https?://(?:www\\.)?linkedin\\.com/in/[a-zA-Z0-9\\-]+/?$'\n",
    "    \n",
    "    if re.match(linkedin_pattern, clean_url):\n",
    "        return clean_url\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def clean_linkedin_urls_in_dataframe(df, column_name='LinkedIn URL'):\n",
    "    \"\"\"\n",
    "    Clean all LinkedIn URLs in a DataFrame column.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing LinkedIn URLs\n",
    "        column_name (str): Name of the column containing LinkedIn URLs\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with cleaned URLs\n",
    "    \"\"\"\n",
    "    if column_name not in df.columns:\n",
    "        print(f\"Column '{column_name}' not found. Available columns: {list(df.columns)}\")\n",
    "        return df\n",
    "    \n",
    "    # Apply cleaning function\n",
    "    df_clean = df.copy()\n",
    "    df_clean[f'{column_name}_cleaned'] = df_clean[column_name].apply(clean_linkedin_url)\n",
    "    \n",
    "    # Show cleaning summary\n",
    "    original_count = df[column_name].notna().sum()\n",
    "    cleaned_count = df_clean[f'{column_name}_cleaned'].notna().sum()\n",
    "    \n",
    "    print(f\"LinkedIn URL cleaning summary:\")\n",
    "    print(f\"  Original URLs (non-null): {original_count}\")\n",
    "    print(f\"  Cleaned valid URLs: {cleaned_count}\")\n",
    "    print(f\"  Removed/Invalid URLs: {original_count - cleaned_count}\")\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "\n",
    "# Apply cleaning to your DataFrame\n",
    "df_cleaned = clean_linkedin_urls_in_dataframe(df, 'LinkedIn URL')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3c8c4c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data cleaning summary:\n",
      "  Original DataFrame: 1102 rows\n",
      "  After dropping N/A LinkedIn URLs: 857 rows\n",
      "  After removing duplicates: 819 rows\n",
      "  Total removed: 283 rows\n",
      "\n",
      "Final dataset shape: (819, 8)\n",
      "Unique LinkedIn URLs for enrichment: 819\n",
      "\n",
      "Preview of final cleaned data:\n",
      "                                         LinkedIn URL  \\\n",
      "0   https://www.linkedin.com/in/sameh-suleiman-asa...   \n",
      "1   https://www.linkedin.com/in/danburnsmena?miniP...   \n",
      "3   https://www.linkedin.com/in/vicky-symonds-1356...   \n",
      "4   https://www.linkedin.com/in/hanine-elchayeb-ci...   \n",
      "5   https://www.linkedin.com/in/amany-ahmed-7bb000...   \n",
      "7   https://www.linkedin.com/in/priya-aliju-43a013...   \n",
      "8   https://www.linkedin.com/in/jamila-al-hosani-5...   \n",
      "9   https://www.linkedin.com/in/shreya-s-2ab14982?...   \n",
      "10  https://www.linkedin.com/in/malmee-jayawardana...   \n",
      "12  https://www.linkedin.com/in/prathibha-prakash-...   \n",
      "\n",
      "                                 LinkedIn URL_cleaned  \n",
      "0   https://www.linkedin.com/in/sameh-suleiman-asa...  \n",
      "1            https://www.linkedin.com/in/danburnsmena  \n",
      "3   https://www.linkedin.com/in/vicky-symonds-1356...  \n",
      "4   https://www.linkedin.com/in/hanine-elchayeb-ci...  \n",
      "5    https://www.linkedin.com/in/amany-ahmed-7bb00012  \n",
      "7    https://www.linkedin.com/in/priya-aliju-43a01327  \n",
      "8   https://www.linkedin.com/in/jamila-al-hosani-5...  \n",
      "9       https://www.linkedin.com/in/shreya-s-2ab14982  \n",
      "10  https://www.linkedin.com/in/malmee-jayawardana...  \n",
      "12  https://www.linkedin.com/in/prathibha-prakash-...  \n"
     ]
    }
   ],
   "source": [
    "# Drop rows where LinkedIn URL_cleaned is None/NaN and remove duplicates\n",
    "df_final = df_cleaned.dropna(subset=['LinkedIn URL_cleaned']).drop_duplicates(subset=['LinkedIn URL_cleaned'])\n",
    "\n",
    "print(f\"Data cleaning summary:\")\n",
    "print(f\"  Original DataFrame: {len(df_cleaned)} rows\")\n",
    "print(f\"  After dropping N/A LinkedIn URLs: {len(df_cleaned.dropna(subset=['LinkedIn URL_cleaned']))} rows\")\n",
    "print(f\"  After removing duplicates: {len(df_final)} rows\")\n",
    "print(f\"  Total removed: {len(df_cleaned) - len(df_final)} rows\")\n",
    "\n",
    "# Show the final cleaned dataset\n",
    "print(f\"\\nFinal dataset shape: {df_final.shape}\")\n",
    "print(f\"Unique LinkedIn URLs for enrichment: {df_final['LinkedIn URL_cleaned'].nunique()}\")\n",
    "\n",
    "# Preview the final data\n",
    "print(\"\\nPreview of final cleaned data:\")\n",
    "print(df_final[['LinkedIn URL', 'LinkedIn URL_cleaned']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f645a682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Name</th>\n",
       "      <th>LinkedIn URL</th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Current Job</th>\n",
       "      <th>Location</th>\n",
       "      <th>Keyword</th>\n",
       "      <th>LinkedIn URL_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cab26bc4f6938a026232f3a07a34b9ec</td>\n",
       "      <td>Sameh Suleiman Asaad</td>\n",
       "      <td>https://www.linkedin.com/in/sameh-suleiman-asa...</td>\n",
       "      <td>Talent Acquisition Specialist at Abu Dhabi Com...</td>\n",
       "      <td>Current: Talent Acquisition Specialist at Abu ...</td>\n",
       "      <td>Dubai, United Arab Emirates</td>\n",
       "      <td>Abu%20Dhabi%20Commercial%20Bank</td>\n",
       "      <td>https://www.linkedin.com/in/sameh-suleiman-asa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8c9e220c18229855a14dfb66a247b926</td>\n",
       "      <td>Daniel Burns</td>\n",
       "      <td>https://www.linkedin.com/in/danburnsmena?miniP...</td>\n",
       "      <td>Head of Talent Acquisition - Abu Dhabi Commerc...</td>\n",
       "      <td>Current: Senior Head of Talent Acquisition at ...</td>\n",
       "      <td>Abu Dhabi</td>\n",
       "      <td>Abu%20Dhabi%20Commercial%20Bank</td>\n",
       "      <td>https://www.linkedin.com/in/danburnsmena</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17637d89776de54a1ad745cd14cb69ba</td>\n",
       "      <td>Vicky Symonds</td>\n",
       "      <td>https://www.linkedin.com/in/vicky-symonds-1356...</td>\n",
       "      <td>Talent Acquisition Specialist at Abu Dhabi Com...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United Arab Emirates</td>\n",
       "      <td>Abu%20Dhabi%20Commercial%20Bank</td>\n",
       "      <td>https://www.linkedin.com/in/vicky-symonds-1356...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>149caf8c53820f2e74304a4893cde9b9</td>\n",
       "      <td>Hanine ElChayeb, CIPD</td>\n",
       "      <td>https://www.linkedin.com/in/hanine-elchayeb-ci...</td>\n",
       "      <td>Senior Team Leader, Talent Acquisition at Abu ...</td>\n",
       "      <td>Past: Recruitment Officer at Abu Dhabi Islamic...</td>\n",
       "      <td>United Arab Emirates</td>\n",
       "      <td>Abu%20Dhabi%20Commercial%20Bank</td>\n",
       "      <td>https://www.linkedin.com/in/hanine-elchayeb-ci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1fe77b32cd4e4dbf9cd6ca3b165d9fde</td>\n",
       "      <td>Amany Ahmed</td>\n",
       "      <td>https://www.linkedin.com/in/amany-ahmed-7bb000...</td>\n",
       "      <td>Recruitment Senior Manager at Abu Dhabi Commer...</td>\n",
       "      <td>Current: Recruitment Senior Manager at Abu Dha...</td>\n",
       "      <td>New Cairo</td>\n",
       "      <td>Abu%20Dhabi%20Commercial%20Bank</td>\n",
       "      <td>https://www.linkedin.com/in/amany-ahmed-7bb00012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1047</th>\n",
       "      <td>892710bd83292a9b309fe482e28a537e</td>\n",
       "      <td>Bassel Eid, PMP, ORM, IOC</td>\n",
       "      <td>https://www.linkedin.com/in/basseleid?miniProf...</td>\n",
       "      <td>Executive Manager - Operational Resilience</td>\n",
       "      <td>Current: Executive Manager - Operational Resil...</td>\n",
       "      <td>Abu Dhabi Emirate, United Arab Emirates</td>\n",
       "      <td>Abu%20Dhabi%20Commercial%20Bank</td>\n",
       "      <td>https://www.linkedin.com/in/basseleid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1049</th>\n",
       "      <td>52e4671b35a9f08dde3e0354409b2cdc</td>\n",
       "      <td>RUBEN HERNANDEZ</td>\n",
       "      <td>https://www.linkedin.com/in/rubenhernandezjr2?...</td>\n",
       "      <td>Experienced Banking Professional</td>\n",
       "      <td>Current: Customer Service Officer at Abu Dhabi...</td>\n",
       "      <td>United Arab Emirates</td>\n",
       "      <td>Abu%20Dhabi%20Commercial%20Bank</td>\n",
       "      <td>https://www.linkedin.com/in/rubenhernandezjr2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1050</th>\n",
       "      <td>8e5bb4ac57578805f7105c9c2cdcd6ec</td>\n",
       "      <td>Emad Asmar</td>\n",
       "      <td>https://www.linkedin.com/in/emad-asmar-3373361...</td>\n",
       "      <td>Senior Relationship Manager at Abu Dhabi Comme...</td>\n",
       "      <td>Past: Assistant Credit Manager at Commercial B...</td>\n",
       "      <td>United Arab Emirates</td>\n",
       "      <td>Abu%20Dhabi%20Commercial%20Bank</td>\n",
       "      <td>https://www.linkedin.com/in/emad-asmar-337336104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051</th>\n",
       "      <td>136b7c2956888f1970fe3008f8b8ae5e</td>\n",
       "      <td>Sreejith Poduvath</td>\n",
       "      <td>https://www.linkedin.com/in/sreejith-poduvath-...</td>\n",
       "      <td>Relationship Manager</td>\n",
       "      <td>Current: Relationship Manager at Abu Dhabi Com...</td>\n",
       "      <td>Dubai, United Arab Emirates</td>\n",
       "      <td>Abu%20Dhabi%20Commercial%20Bank</td>\n",
       "      <td>https://www.linkedin.com/in/sreejith-poduvath-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1052</th>\n",
       "      <td>5c1c569ac4d5234571912ba68e446213</td>\n",
       "      <td>Sudhir Dasgupta</td>\n",
       "      <td>https://www.linkedin.com/in/sudhir-dasgupta-44...</td>\n",
       "      <td>Head - Retail Products and Marketing at Abu Dh...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United Arab Emirates</td>\n",
       "      <td>Abu%20Dhabi%20Commercial%20Bank</td>\n",
       "      <td>https://www.linkedin.com/in/sudhir-dasgupta-44...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>819 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    ID                       Name  \\\n",
       "0     cab26bc4f6938a026232f3a07a34b9ec       Sameh Suleiman Asaad   \n",
       "1     8c9e220c18229855a14dfb66a247b926               Daniel Burns   \n",
       "3     17637d89776de54a1ad745cd14cb69ba              Vicky Symonds   \n",
       "4     149caf8c53820f2e74304a4893cde9b9      Hanine ElChayeb, CIPD   \n",
       "5     1fe77b32cd4e4dbf9cd6ca3b165d9fde                Amany Ahmed   \n",
       "...                                ...                        ...   \n",
       "1047  892710bd83292a9b309fe482e28a537e  Bassel Eid, PMP, ORM, IOC   \n",
       "1049  52e4671b35a9f08dde3e0354409b2cdc            RUBEN HERNANDEZ   \n",
       "1050  8e5bb4ac57578805f7105c9c2cdcd6ec                 Emad Asmar   \n",
       "1051  136b7c2956888f1970fe3008f8b8ae5e          Sreejith Poduvath   \n",
       "1052  5c1c569ac4d5234571912ba68e446213            Sudhir Dasgupta   \n",
       "\n",
       "                                           LinkedIn URL  \\\n",
       "0     https://www.linkedin.com/in/sameh-suleiman-asa...   \n",
       "1     https://www.linkedin.com/in/danburnsmena?miniP...   \n",
       "3     https://www.linkedin.com/in/vicky-symonds-1356...   \n",
       "4     https://www.linkedin.com/in/hanine-elchayeb-ci...   \n",
       "5     https://www.linkedin.com/in/amany-ahmed-7bb000...   \n",
       "...                                                 ...   \n",
       "1047  https://www.linkedin.com/in/basseleid?miniProf...   \n",
       "1049  https://www.linkedin.com/in/rubenhernandezjr2?...   \n",
       "1050  https://www.linkedin.com/in/emad-asmar-3373361...   \n",
       "1051  https://www.linkedin.com/in/sreejith-poduvath-...   \n",
       "1052  https://www.linkedin.com/in/sudhir-dasgupta-44...   \n",
       "\n",
       "                                              Job Title  \\\n",
       "0     Talent Acquisition Specialist at Abu Dhabi Com...   \n",
       "1     Head of Talent Acquisition - Abu Dhabi Commerc...   \n",
       "3     Talent Acquisition Specialist at Abu Dhabi Com...   \n",
       "4     Senior Team Leader, Talent Acquisition at Abu ...   \n",
       "5     Recruitment Senior Manager at Abu Dhabi Commer...   \n",
       "...                                                 ...   \n",
       "1047         Executive Manager - Operational Resilience   \n",
       "1049                   Experienced Banking Professional   \n",
       "1050  Senior Relationship Manager at Abu Dhabi Comme...   \n",
       "1051                               Relationship Manager   \n",
       "1052  Head - Retail Products and Marketing at Abu Dh...   \n",
       "\n",
       "                                            Current Job  \\\n",
       "0     Current: Talent Acquisition Specialist at Abu ...   \n",
       "1     Current: Senior Head of Talent Acquisition at ...   \n",
       "3                                                   NaN   \n",
       "4     Past: Recruitment Officer at Abu Dhabi Islamic...   \n",
       "5     Current: Recruitment Senior Manager at Abu Dha...   \n",
       "...                                                 ...   \n",
       "1047  Current: Executive Manager - Operational Resil...   \n",
       "1049  Current: Customer Service Officer at Abu Dhabi...   \n",
       "1050  Past: Assistant Credit Manager at Commercial B...   \n",
       "1051  Current: Relationship Manager at Abu Dhabi Com...   \n",
       "1052                                                NaN   \n",
       "\n",
       "                                     Location  \\\n",
       "0                 Dubai, United Arab Emirates   \n",
       "1                                   Abu Dhabi   \n",
       "3                        United Arab Emirates   \n",
       "4                        United Arab Emirates   \n",
       "5                                   New Cairo   \n",
       "...                                       ...   \n",
       "1047  Abu Dhabi Emirate, United Arab Emirates   \n",
       "1049                     United Arab Emirates   \n",
       "1050                     United Arab Emirates   \n",
       "1051              Dubai, United Arab Emirates   \n",
       "1052                     United Arab Emirates   \n",
       "\n",
       "                              Keyword  \\\n",
       "0     Abu%20Dhabi%20Commercial%20Bank   \n",
       "1     Abu%20Dhabi%20Commercial%20Bank   \n",
       "3     Abu%20Dhabi%20Commercial%20Bank   \n",
       "4     Abu%20Dhabi%20Commercial%20Bank   \n",
       "5     Abu%20Dhabi%20Commercial%20Bank   \n",
       "...                               ...   \n",
       "1047  Abu%20Dhabi%20Commercial%20Bank   \n",
       "1049  Abu%20Dhabi%20Commercial%20Bank   \n",
       "1050  Abu%20Dhabi%20Commercial%20Bank   \n",
       "1051  Abu%20Dhabi%20Commercial%20Bank   \n",
       "1052  Abu%20Dhabi%20Commercial%20Bank   \n",
       "\n",
       "                                   LinkedIn URL_cleaned  \n",
       "0     https://www.linkedin.com/in/sameh-suleiman-asa...  \n",
       "1              https://www.linkedin.com/in/danburnsmena  \n",
       "3     https://www.linkedin.com/in/vicky-symonds-1356...  \n",
       "4     https://www.linkedin.com/in/hanine-elchayeb-ci...  \n",
       "5      https://www.linkedin.com/in/amany-ahmed-7bb00012  \n",
       "...                                                 ...  \n",
       "1047              https://www.linkedin.com/in/basseleid  \n",
       "1049      https://www.linkedin.com/in/rubenhernandezjr2  \n",
       "1050   https://www.linkedin.com/in/emad-asmar-337336104  \n",
       "1051  https://www.linkedin.com/in/sreejith-poduvath-...  \n",
       "1052  https://www.linkedin.com/in/sudhir-dasgupta-44...  \n",
       "\n",
       "[819 rows x 8 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1f30e97d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Name</th>\n",
       "      <th>LinkedIn URL</th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Current Job</th>\n",
       "      <th>Location</th>\n",
       "      <th>Keyword</th>\n",
       "      <th>LinkedIn URL_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cab26bc4f6938a026232f3a07a34b9ec</td>\n",
       "      <td>Sameh Suleiman Asaad</td>\n",
       "      <td>https://www.linkedin.com/in/sameh-suleiman-asa...</td>\n",
       "      <td>Talent Acquisition Specialist at Abu Dhabi Com...</td>\n",
       "      <td>Current: Talent Acquisition Specialist at Abu ...</td>\n",
       "      <td>Dubai, United Arab Emirates</td>\n",
       "      <td>Abu%20Dhabi%20Commercial%20Bank</td>\n",
       "      <td>https://www.linkedin.com/in/sameh-suleiman-asa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8c9e220c18229855a14dfb66a247b926</td>\n",
       "      <td>Daniel Burns</td>\n",
       "      <td>https://www.linkedin.com/in/danburnsmena?miniP...</td>\n",
       "      <td>Head of Talent Acquisition - Abu Dhabi Commerc...</td>\n",
       "      <td>Current: Senior Head of Talent Acquisition at ...</td>\n",
       "      <td>Abu Dhabi</td>\n",
       "      <td>Abu%20Dhabi%20Commercial%20Bank</td>\n",
       "      <td>https://www.linkedin.com/in/danburnsmena</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17637d89776de54a1ad745cd14cb69ba</td>\n",
       "      <td>Vicky Symonds</td>\n",
       "      <td>https://www.linkedin.com/in/vicky-symonds-1356...</td>\n",
       "      <td>Talent Acquisition Specialist at Abu Dhabi Com...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United Arab Emirates</td>\n",
       "      <td>Abu%20Dhabi%20Commercial%20Bank</td>\n",
       "      <td>https://www.linkedin.com/in/vicky-symonds-1356...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 ID                  Name  \\\n",
       "0  cab26bc4f6938a026232f3a07a34b9ec  Sameh Suleiman Asaad   \n",
       "1  8c9e220c18229855a14dfb66a247b926          Daniel Burns   \n",
       "3  17637d89776de54a1ad745cd14cb69ba         Vicky Symonds   \n",
       "\n",
       "                                        LinkedIn URL  \\\n",
       "0  https://www.linkedin.com/in/sameh-suleiman-asa...   \n",
       "1  https://www.linkedin.com/in/danburnsmena?miniP...   \n",
       "3  https://www.linkedin.com/in/vicky-symonds-1356...   \n",
       "\n",
       "                                           Job Title  \\\n",
       "0  Talent Acquisition Specialist at Abu Dhabi Com...   \n",
       "1  Head of Talent Acquisition - Abu Dhabi Commerc...   \n",
       "3  Talent Acquisition Specialist at Abu Dhabi Com...   \n",
       "\n",
       "                                         Current Job  \\\n",
       "0  Current: Talent Acquisition Specialist at Abu ...   \n",
       "1  Current: Senior Head of Talent Acquisition at ...   \n",
       "3                                                NaN   \n",
       "\n",
       "                      Location                          Keyword  \\\n",
       "0  Dubai, United Arab Emirates  Abu%20Dhabi%20Commercial%20Bank   \n",
       "1                    Abu Dhabi  Abu%20Dhabi%20Commercial%20Bank   \n",
       "3         United Arab Emirates  Abu%20Dhabi%20Commercial%20Bank   \n",
       "\n",
       "                                LinkedIn URL_cleaned  \n",
       "0  https://www.linkedin.com/in/sameh-suleiman-asa...  \n",
       "1           https://www.linkedin.com/in/danburnsmena  \n",
       "3  https://www.linkedin.com/in/vicky-symonds-1356...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e537a4",
   "metadata": {},
   "source": [
    "## Apollo.io API for enrichment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640367ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Apollo enrichment with first 10 rows using CONCURRENT processing...\n",
      "Test DataFrame shape: (10, 8)\n",
      "LinkedIn URLs to test:\n",
      "  1. https://www.linkedin.com/in/sameh-suleiman-asaad-22a8b359\n",
      "  2. https://www.linkedin.com/in/danburnsmena\n",
      "  3. https://www.linkedin.com/in/vicky-symonds-1356b4134\n",
      "  4. https://www.linkedin.com/in/hanine-elchayeb-cipd-a7486a63\n",
      "  5. https://www.linkedin.com/in/amany-ahmed-7bb00012\n",
      "  6. https://www.linkedin.com/in/priya-aliju-43a01327\n",
      "  7. https://www.linkedin.com/in/jamila-al-hosani-5ab783146\n",
      "  8. https://www.linkedin.com/in/shreya-s-2ab14982\n",
      "  9. https://www.linkedin.com/in/malmee-jayawardana-0704b14a\n",
      "  10. https://www.linkedin.com/in/prathibha-prakash-04795322\n",
      "Found 10 unique LinkedIn URLs to enrich\n",
      "Using 5 concurrent workers with 0.5s delay\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da9caac1896d4a50afab20e2f71f52e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Enriching leads:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Enrichment completed!\n",
      "Saved 10 enriched records to enriched_leads_test_10_concurrent.csv\n",
      "Saved raw Apollo data to enriched_leads_test_10_concurrent_raw.json\n",
      "Successful enrichments: 10\n",
      "Failed enrichments: 0\n",
      "\n",
      "Test enrichment results:\n",
      "            full_name                        email  \\\n",
      "0     Hanine Elchayeb                         None   \n",
      "1         Amany Ahmed         amany.ahmed@adcb.com   \n",
      "2         Sameh Asaad         sameh.asaad@adcb.com   \n",
      "3       Vicky Symonds       vicky.symonds@adcb.com   \n",
      "4     Shreya Sudhakar            shreya.s@adcb.com   \n",
      "5         Priya Aliju             priya.a@adcb.com   \n",
      "6  Malmee Jayawardana  malmee.jayawardana@adcb.com   \n",
      "7        Daniel Burns           dan.burns@adcb.com   \n",
      "8       Jamila Hosani       jamila.hosani@adcb.com   \n",
      "9   Prathibha Prakash   prathibha.prakash@adcb.com   \n",
      "\n",
      "                                    title          organization_name  \\\n",
      "0  Senior Team Leader, Talent Acquisition  Abu Dhabi Commercial Bank   \n",
      "1              Recruitment Senior Manager  Abu Dhabi Commercial Bank   \n",
      "2           Talent Acquisition Specialist  Abu Dhabi Commercial Bank   \n",
      "3           Talent Acquisition Specialist  Abu Dhabi Commercial Bank   \n",
      "4           Talent Acquisition Specialist  Abu Dhabi Commercial Bank   \n",
      "5   Senior Team Lead - Talent Acquisition  Abu Dhabi Commercial Bank   \n",
      "6   Talent Acquisition Support Specialist  Abu Dhabi Commercial Bank   \n",
      "7       Senior Head of Talent Acquisition  Abu Dhabi Commercial Bank   \n",
      "8                 Head Talent Acquisition  Abu Dhabi Commercial Bank   \n",
      "9                      Talent Acquisition  Abu Dhabi Commercial Bank   \n",
      "\n",
      "             city               country  \n",
      "0            None  United Arab Emirates  \n",
      "1  New Cairo City                 Egypt  \n",
      "2           Dubai  United Arab Emirates  \n",
      "3            None  United Arab Emirates  \n",
      "4            None  United Arab Emirates  \n",
      "5           Dubai  United Arab Emirates  \n",
      "6           Dubai  United Arab Emirates  \n",
      "7       Abu Dhabi  United Arab Emirates  \n",
      "8       Abu Dhabi  United Arab Emirates  \n",
      "9           Dubai  United Arab Emirates  \n",
      "\n",
      "Test completed!\n",
      "Processed data saved to: 'enriched_leads_test_10_concurrent.csv'\n",
      "Raw Apollo data saved to: 'enriched_leads_test_10_concurrent_raw.json'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from time import sleep\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "# Apollo API key from .env\n",
    "APOLLO_API_KEY = os.getenv(\"apollo_key\")\n",
    "\n",
    "def enrich_lead_with_linkedin(linkedin_url, reveal_personal_emails=False, reveal_phone_number=False):\n",
    "    \"\"\"\n",
    "    Enrich a lead using the Apollo API with a LinkedIn URL.\n",
    "    \"\"\"\n",
    "    url = \"https://api.apollo.io/api/v1/people/match\"\n",
    "    headers = {\n",
    "        \"accept\": \"application/json\",\n",
    "        \"Cache-Control\": \"no-cache\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"x-api-key\": APOLLO_API_KEY\n",
    "    }\n",
    "    payload = {\n",
    "        \"linkedin_url\": linkedin_url,\n",
    "        \"reveal_personal_emails\": reveal_personal_emails,\n",
    "        \"reveal_phone_number\": reveal_phone_number\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=payload)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "def flatten_person_data(apollo_result):\n",
    "    \"\"\"\n",
    "    Flatten the Apollo API result into a flat dictionary for CSV export.\n",
    "    Returns both flattened data and raw data.\n",
    "    \"\"\"\n",
    "    if 'error' in apollo_result:\n",
    "        return {'error': apollo_result['error']}, apollo_result\n",
    "    \n",
    "    person = apollo_result.get('person', {})\n",
    "    org = person.get('organization', {})\n",
    "    \n",
    "    # Extract main person data\n",
    "    flat_data = {\n",
    "        'apollo_id': person.get('id'),\n",
    "        'first_name': person.get('first_name'),\n",
    "        'last_name': person.get('last_name'),\n",
    "        'full_name': person.get('name'),\n",
    "        'email': person.get('email'),\n",
    "        'email_status': person.get('email_status'),\n",
    "        'linkedin_url': person.get('linkedin_url'),\n",
    "        'title': person.get('title'),\n",
    "        'headline': person.get('headline'),\n",
    "        'photo_url': person.get('photo_url'),\n",
    "        'city': person.get('city'),\n",
    "        'state': person.get('state'),\n",
    "        'country': person.get('country'),\n",
    "        'seniority': person.get('seniority'),\n",
    "        \n",
    "        # Organization data\n",
    "        'organization_id': org.get('id'),\n",
    "        'organization_name': org.get('name'),\n",
    "        'organization_website': org.get('website_url'),\n",
    "        'organization_linkedin': org.get('linkedin_url'),\n",
    "        'organization_phone': org.get('phone'),\n",
    "        'organization_industry': org.get('industry'),\n",
    "        'organization_employees': org.get('estimated_num_employees'),\n",
    "        'organization_revenue': org.get('annual_revenue'),\n",
    "        'organization_founded': org.get('founded_year'),\n",
    "        'organization_description': org.get('short_description'),\n",
    "        \n",
    "        # Current employment (most recent)\n",
    "        'current_company': None,\n",
    "        'current_title': None,\n",
    "        'current_start_date': None,\n",
    "    }\n",
    "    \n",
    "    # Extract current employment info\n",
    "    employment_history = person.get('employment_history', [])\n",
    "    if employment_history:\n",
    "        current_job = next((job for job in employment_history if job.get('current')), employment_history[0])\n",
    "        flat_data['current_company'] = current_job.get('organization_name')\n",
    "        flat_data['current_title'] = current_job.get('title')\n",
    "        flat_data['current_start_date'] = current_job.get('start_date')\n",
    "    \n",
    "    # Extract departments and functions\n",
    "    departments = person.get('departments', [])\n",
    "    functions = person.get('functions', [])\n",
    "    flat_data['departments'] = ', '.join(departments) if departments else None\n",
    "    flat_data['functions'] = ', '.join(functions) if functions else None\n",
    "    \n",
    "    return flat_data, apollo_result\n",
    "\n",
    "def process_single_url(linkedin_url):\n",
    "    \"\"\"\n",
    "    Process a single LinkedIn URL - wrapper for concurrent processing.\n",
    "    \"\"\"\n",
    "    result = enrich_lead_with_linkedin(linkedin_url)\n",
    "    flat_result, raw_result = flatten_person_data(result)\n",
    "    flat_result['original_linkedin_url'] = linkedin_url\n",
    "    \n",
    "    # Add raw data with linkedin_url as key for easy reference\n",
    "    raw_data = {\n",
    "        'original_linkedin_url': linkedin_url,\n",
    "        'apollo_raw': raw_result\n",
    "    }\n",
    "    \n",
    "    return flat_result, raw_data\n",
    "\n",
    "def enrich_linkedin_urls_from_dataframe_concurrent(df_input, linkedin_column='LinkedIn URL_cleaned', \n",
    "                                                 output_csv_path='enriched_leads_concurrent.csv', \n",
    "                                                 max_workers=3, delay=0.5):\n",
    "    \"\"\"\n",
    "    Read LinkedIn URLs from a DataFrame and enrich them using Apollo API with concurrency.\n",
    "    \n",
    "    Args:\n",
    "        df_input (pd.DataFrame): DataFrame with LinkedIn URLs\n",
    "        linkedin_column (str): Column name containing LinkedIn URLs\n",
    "        output_csv_path (str): Path for output CSV with enriched data\n",
    "        max_workers (int): Maximum number of concurrent workers\n",
    "        delay (float): Delay between batches in seconds\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if LinkedIn column exists\n",
    "    if linkedin_column not in df_input.columns:\n",
    "        print(f\"Column '{linkedin_column}' not found. Available columns: {list(df_input.columns)}\")\n",
    "        return None, None\n",
    "    \n",
    "    # Get LinkedIn URLs (drop null values)\n",
    "    linkedin_urls = df_input[linkedin_column].dropna().unique()\n",
    "    print(f\"Found {len(linkedin_urls)} unique LinkedIn URLs to enrich\")\n",
    "    print(f\"Using {max_workers} concurrent workers with {delay}s delay\")\n",
    "    \n",
    "    enriched_data = []\n",
    "    raw_data_list = []\n",
    "    \n",
    "    # Use ThreadPoolExecutor for concurrent processing\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit all tasks\n",
    "        future_to_url = {executor.submit(process_single_url, url): url for url in linkedin_urls}\n",
    "        \n",
    "        # Process completed tasks with progress bar\n",
    "        with tqdm(total=len(linkedin_urls), desc=\"Enriching leads\") as pbar:\n",
    "            for future in as_completed(future_to_url):\n",
    "                url = future_to_url[future]\n",
    "                try:\n",
    "                    flat_result, raw_result = future.result()\n",
    "                    enriched_data.append(flat_result)\n",
    "                    raw_data_list.append(raw_result)\n",
    "                    pbar.set_postfix({'URL': url[:50] + '...' if len(url) > 50 else url})\n",
    "                except Exception as e:\n",
    "                    print(f\"\\nError processing {url}: {e}\")\n",
    "                    # Add error record\n",
    "                    error_record = {'error': str(e), 'original_linkedin_url': url}\n",
    "                    enriched_data.append(error_record)\n",
    "                    raw_data_list.append({'original_linkedin_url': url, 'apollo_raw': {'error': str(e)}})\n",
    "                \n",
    "                pbar.update(1)\n",
    "                \n",
    "                # Sleep to be respectful to API\n",
    "                if delay > 0:\n",
    "                    sleep(delay)\n",
    "    \n",
    "    # Convert to DataFrames and save\n",
    "    df_enriched = pd.DataFrame(enriched_data)\n",
    "    df_enriched.to_csv(output_csv_path, index=False)\n",
    "    \n",
    "    # Save raw data as JSON for complete preservation\n",
    "    raw_json_path = output_csv_path.replace('.csv', '_raw.json')\n",
    "    with open(raw_json_path, 'w') as f:\n",
    "        json.dump(raw_data_list, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nEnrichment completed!\")\n",
    "    print(f\"Saved {len(df_enriched)} enriched records to {output_csv_path}\")\n",
    "    print(f\"Saved raw Apollo data to {raw_json_path}\")\n",
    "    \n",
    "    # Show summary\n",
    "    successful_enrichments = df_enriched[df_enriched['error'].isna()].shape[0] if 'error' in df_enriched.columns else len(df_enriched)\n",
    "    failed_enrichments = len(df_enriched) - successful_enrichments\n",
    "    \n",
    "    print(f\"Successful enrichments: {successful_enrichments}\")\n",
    "    print(f\"Failed enrichments: {failed_enrichments}\")\n",
    "    \n",
    "    return df_enriched, raw_data_list\n",
    "\n",
    "# Clean LinkedIn URLs in the DataFrame\n",
    "for i, url in enumerate(df_final['LinkedIn URL_cleaned'], 1):\n",
    "    print(f\"  {i}. {url}\")\n",
    "\n",
    "# Run CONCURRENT enrichment on test data\n",
    "df_enriched_test, raw_data_test = enrich_linkedin_urls_from_dataframe_concurrent(\n",
    "    df_input=df_final,\n",
    "    linkedin_column='LinkedIn URL_cleaned',\n",
    "    output_csv_path='enriched_leads_abu_dhabi_commercial_bank.csv',\n",
    "    max_workers=5,  # 5 concurrent workers\n",
    "    delay=0.5  # 500ms delay between batches\n",
    ")\n",
    "\n",
    "# Show results\n",
    "if df_enriched_test is not None and not df_enriched_test.empty:\n",
    "    print(\"\\n enrichment results:\")\n",
    "    preview_cols = ['full_name', 'email', 'title', 'organization_name', 'city', 'country']\n",
    "    available_cols = [col for col in preview_cols if col in df_enriched_test.columns]\n",
    "    print(df_enriched_test[available_cols])\n",
    "    \n",
    "    print(f\"\\ncompleted!\")\n",
    "    print(f\"Processed data saved to: 'enriched_leads_abu_dhabi_commercial_bank.csv'\")\n",
    "    print(f\"Raw Apollo data saved to: 'enriched_leads_abu_dhabi_commercial_bank_raw.json'\")\n",
    "else:\n",
    "    print(\" failed - no enriched data returned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d309dab",
   "metadata": {},
   "source": [
    "### Final script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8d5834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting OPTIMIZED Apollo enrichment for 200/min rate limit...\n",
      "📊 Processing 819 LinkedIn URLs\n",
      "📊 Total URLs: 819\n",
      "✅ Already completed: 0\n",
      "⏳ Remaining to process: 819\n",
      "🧠 Smart sequential processing with rate limit awareness\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76da54b484b34a3fa790d81d758f16dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Enriching leads:   0%|          | 0/819 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Rate Limit Status:\n",
      "   Minute: 5.0% used\n",
      "   Hourly: 0.2% used\n",
      "\n",
      "📊 Rate Limit Status:\n",
      "   Minute: 10.0% used\n",
      "   Hourly: 0.4% used\n",
      "\n",
      "📊 Rate Limit Status:\n",
      "   Minute: 15.0% used\n",
      "   Hourly: 0.5% used\n",
      "\n",
      "📊 Rate Limit Status:\n",
      "   Minute: 20.0% used\n",
      "   Hourly: 0.7% used\n",
      "\n",
      "💾 Checkpoint saved at 50 records\n",
      "\n",
      "📊 Rate Limit Status:\n",
      "   Minute: 25.0% used\n",
      "   Hourly: 0.9% used\n",
      "\n",
      "📊 Rate Limit Status:\n",
      "   Minute: 2.0% used\n",
      "   Hourly: 1.0% used\n",
      "\n",
      "📊 Rate Limit Status:\n",
      "   Minute: 7.0% used\n",
      "   Hourly: 1.2% used\n",
      "\n",
      "📊 Rate Limit Status:\n",
      "   Minute: 12.0% used\n",
      "   Hourly: 1.4% used\n",
      "\n",
      "📊 Rate Limit Status:\n",
      "   Minute: 17.0% used\n",
      "   Hourly: 1.5% used\n",
      "\n",
      "💾 Checkpoint saved at 100 records\n",
      "\n",
      "📊 Rate Limit Status:\n",
      "   Minute: 22.0% used\n",
      "   Hourly: 1.7% used\n",
      "\n",
      "📊 Rate Limit Status:\n",
      "   Minute: 27.0% used\n",
      "   Hourly: 1.9% used\n",
      "\n",
      "📊 Rate Limit Status:\n",
      "   Minute: 0.5% used\n",
      "   Hourly: 2.0% used\n",
      "\n",
      "📊 Rate Limit Status:\n",
      "   Minute: 5.5% used\n",
      "   Hourly: 2.2% used\n",
      "\n",
      "📊 Rate Limit Status:\n",
      "   Minute: 10.5% used\n",
      "   Hourly: 2.4% used\n",
      "\n",
      "💾 Checkpoint saved at 150 records\n",
      "\n",
      "📊 Rate Limit Status:\n",
      "   Minute: 15.5% used\n",
      "   Hourly: 2.5% used\n",
      "\n",
      "📊 Rate Limit Status:\n",
      "   Minute: 20.5% used\n",
      "   Hourly: 2.7% used\n",
      "\n",
      "📊 Rate Limit Status:\n",
      "   Minute: 25.5% used\n",
      "   Hourly: 2.9% used\n",
      "\n",
      "📊 Rate Limit Status:\n",
      "   Minute: 30.5% used\n",
      "   Hourly: 3.0% used\n",
      "\n",
      "📊 Rate Limit Status:\n",
      "   Minute: 1.5% used\n",
      "   Hourly: 3.2% used\n",
      "\n",
      "💾 Checkpoint saved at 200 records\n",
      "\n",
      "📊 Rate Limit Status:\n",
      "   Minute: 6.5% used\n",
      "   Hourly: 3.4% used\n",
      "\n",
      "📊 Rate Limit Status:\n",
      "   Minute: 11.5% used\n",
      "   Hourly: 3.5% used\n",
      "\n",
      "📊 Rate Limit Status:\n",
      "   Minute: 16.5% used\n",
      "   Hourly: 3.7% used\n",
      "\n",
      "📊 Rate Limit Status:\n",
      "   Minute: 21.5% used\n",
      "   Hourly: 3.9% used\n",
      "\n",
      "📊 Rate Limit Status:\n",
      "   Minute: 26.5% used\n",
      "   Hourly: 4.0% used\n",
      "\n",
      "💾 Checkpoint saved at 250 records\n",
      "\n",
      "📊 Rate Limit Status:\n",
      "   Minute: 31.5% used\n",
      "   Hourly: 4.2% used\n",
      "\n",
      "📊 Rate Limit Status:\n",
      "   Minute: 4.0% used\n",
      "   Hourly: 4.4% used\n",
      "\n",
      "📊 Rate Limit Status:\n",
      "   Minute: 9.0% used\n",
      "   Hourly: 4.5% used\n",
      "\n",
      "📊 Rate Limit Status:\n",
      "   Minute: 14.0% used\n",
      "   Hourly: 4.7% used\n",
      "\n",
      "📊 Rate Limit Status:\n",
      "   Minute: 19.0% used\n",
      "   Hourly: 4.9% used\n",
      "\n",
      "💾 Checkpoint saved at 300 records\n",
      "\n",
      "📊 Rate Limit Status:\n",
      "   Minute: 24.0% used\n",
      "   Hourly: 5.0% used\n",
      "\n",
      "📊 Rate Limit Status:\n",
      "   Minute: 29.0% used\n",
      "   Hourly: 5.2% used\n",
      "\n",
      "📊 Rate Limit Status:\n",
      "   Minute: 34.0% used\n",
      "   Hourly: 5.4% used\n",
      "\n",
      "📊 Rate Limit Status:\n",
      "   Minute: 5.0% used\n",
      "   Hourly: 5.5% used\n",
      "\n",
      "📊 Rate Limit Status:\n",
      "   Minute: 10.0% used\n",
      "   Hourly: 5.7% used\n",
      "\n",
      "💾 Checkpoint saved at 350 records\n",
      "\n",
      "📊 Rate Limit Status:\n",
      "   Minute: 15.0% used\n",
      "   Hourly: 5.9% used\n",
      "\n",
      "📊 Rate Limit Status:\n",
      "   Minute: 20.0% used\n",
      "   Hourly: 6.0% used\n",
      "\n",
      "📊 Rate Limit Status:\n",
      "   Minute: 25.0% used\n",
      "   Hourly: 6.2% used\n",
      "\n",
      "📊 Rate Limit Status:\n",
      "   Minute: 30.0% used\n",
      "   Hourly: 6.4% used\n",
      "\n",
      "📊 Rate Limit Status:\n",
      "   Minute: 1.5% used\n",
      "   Hourly: 6.5% used\n",
      "\n",
      "💾 Checkpoint saved at 400 records\n",
      "\n",
      "📊 Rate Limit Status:\n",
      "   Minute: 6.5% used\n",
      "   Hourly: 6.7% used\n",
      "\n",
      "📊 Rate Limit Status:\n",
      "   Minute: 11.5% used\n",
      "   Hourly: 6.9% used\n",
      "\n",
      "📊 Rate Limit Status:\n",
      "   Minute: 16.5% used\n",
      "   Hourly: 7.0% used\n",
      "\n",
      "📊 Rate Limit Status:\n",
      "   Minute: 21.5% used\n",
      "   Hourly: 7.2% used\n",
      "\n",
      "📊 Rate Limit Status:\n",
      "   Minute: 26.5% used\n",
      "   Hourly: 7.4% used\n",
      "\n",
      "💾 Checkpoint saved at 450 records\n",
      "\n",
      "📊 Rate Limit Status:\n",
      "   Minute: 31.5% used\n",
      "   Hourly: 7.5% used\n",
      "\n",
      "📊 Rate Limit Status:\n",
      "   Minute: 1.0% used\n",
      "   Hourly: 7.7% used\n",
      "\n",
      "📊 Rate Limit Status:\n",
      "   Minute: 6.0% used\n",
      "   Hourly: 7.9% used\n",
      "\n",
      "📊 Rate Limit Status:\n",
      "   Minute: 11.0% used\n",
      "   Hourly: 8.0% used\n",
      "\n",
      "📊 Rate Limit Status:\n",
      "   Minute: 16.0% used\n",
      "   Hourly: 8.2% used\n",
      "\n",
      "💾 Checkpoint saved at 500 records\n",
      "\n",
      "📊 Rate Limit Status:\n",
      "   Minute: 21.0% used\n",
      "   Hourly: 8.4% used\n",
      "\n",
      "📊 Rate Limit Status:\n",
      "   Minute: 26.0% used\n",
      "   Hourly: 8.5% used\n",
      "\n",
      "📊 Rate Limit Status:\n",
      "   Minute: 31.0% used\n",
      "   Hourly: 8.7% used\n",
      "\n",
      "📊 Rate Limit Status:\n",
      "   Minute: 1.5% used\n",
      "   Hourly: 8.9% used\n",
      "\n",
      "📊 Rate Limit Status:\n",
      "   Minute: 6.5% used\n",
      "   Hourly: 9.0% used\n",
      "\n",
      "💾 Checkpoint saved at 550 records\n",
      "\n",
      "📊 Rate Limit Status:\n",
      "   Minute: 11.5% used\n",
      "   Hourly: 9.2% used\n",
      "\n",
      "📊 Rate Limit Status:\n",
      "   Minute: 16.5% used\n",
      "   Hourly: 9.4% used\n",
      "\n",
      "📊 Rate Limit Status:\n",
      "   Minute: 21.5% used\n",
      "   Hourly: 9.5% used\n",
      "\n",
      "📊 Rate Limit Status:\n",
      "   Minute: 26.5% used\n",
      "   Hourly: 9.7% used\n",
      "\n",
      "📊 Rate Limit Status:\n",
      "   Minute: 31.5% used\n",
      "   Hourly: 9.9% used\n",
      "\n",
      "💾 Checkpoint saved at 600 records\n",
      "\n",
      "📊 Rate Limit Status:\n",
      "   Minute: 1.5% used\n",
      "   Hourly: 10.0% used\n",
      "\n",
      "📊 Rate Limit Status:\n",
      "   Minute: 6.5% used\n",
      "   Hourly: 10.2% used\n",
      "\n",
      "📊 Rate Limit Status:\n",
      "   Minute: 11.5% used\n",
      "   Hourly: 10.4% used\n",
      "\n",
      "📊 Rate Limit Status:\n",
      "   Minute: 16.5% used\n",
      "   Hourly: 10.5% used\n",
      "\n",
      "📊 Rate Limit Status:\n",
      "   Minute: 21.5% used\n",
      "   Hourly: 10.7% used\n",
      "\n",
      "💾 Checkpoint saved at 650 records\n",
      "\n",
      "📊 Rate Limit Status:\n",
      "   Minute: 26.5% used\n",
      "   Hourly: 10.9% used\n",
      "\n",
      "📊 Rate Limit Status:\n",
      "   Minute: 31.5% used\n",
      "   Hourly: 11.0% used\n",
      "\n",
      "📊 Rate Limit Status:\n",
      "   Minute: 2.0% used\n",
      "   Hourly: 11.2% used\n",
      "\n",
      "📊 Rate Limit Status:\n",
      "   Minute: 7.0% used\n",
      "   Hourly: 11.4% used\n",
      "\n",
      "📊 Rate Limit Status:\n",
      "   Minute: 12.0% used\n",
      "   Hourly: 11.5% used\n",
      "\n",
      "💾 Checkpoint saved at 700 records\n",
      "\n",
      "📊 Rate Limit Status:\n",
      "   Minute: 17.0% used\n",
      "   Hourly: 11.7% used\n",
      "\n",
      "📊 Rate Limit Status:\n",
      "   Minute: 22.0% used\n",
      "   Hourly: 11.9% used\n",
      "\n",
      "📊 Rate Limit Status:\n",
      "   Minute: 27.0% used\n",
      "   Hourly: 12.0% used\n",
      "\n",
      "📊 Rate Limit Status:\n",
      "   Minute: 32.0% used\n",
      "   Hourly: 12.2% used\n",
      "\n",
      "📊 Rate Limit Status:\n",
      "   Minute: 1.5% used\n",
      "   Hourly: 12.4% used\n",
      "\n",
      "💾 Checkpoint saved at 750 records\n",
      "\n",
      "📊 Rate Limit Status:\n",
      "   Minute: 6.5% used\n",
      "   Hourly: 12.5% used\n",
      "\n",
      "📊 Rate Limit Status:\n",
      "   Minute: 11.5% used\n",
      "   Hourly: 12.7% used\n",
      "\n",
      "📊 Rate Limit Status:\n",
      "   Minute: 16.5% used\n",
      "   Hourly: 12.9% used\n",
      "\n",
      "📊 Rate Limit Status:\n",
      "   Minute: 21.5% used\n",
      "   Hourly: 13.0% used\n",
      "\n",
      "📊 Rate Limit Status:\n",
      "   Minute: 26.5% used\n",
      "   Hourly: 13.2% used\n",
      "\n",
      "💾 Checkpoint saved at 800 records\n",
      "\n",
      "📊 Rate Limit Status:\n",
      "   Minute: 31.5% used\n",
      "   Hourly: 13.4% used\n",
      "\n",
      "📊 Rate Limit Status:\n",
      "   Minute: 36.5% used\n",
      "   Hourly: 13.5% used\n",
      "\n",
      "🎉 Enrichment completed!\n",
      "📁 Final results saved to: enriched_leads_abu_dhabi_commercial_bank_optimized_complete_20250604_190338.csv\n",
      "📁 Latest version: enriched_leads_abu_dhabi_commercial_bank_optimized.csv\n",
      "📁 Raw data: enriched_leads_abu_dhabi_commercial_bank_optimized_complete_20250604_190338_raw.json\n",
      "\n",
      "📊 Final Rate Limit Status:\n",
      "   Hourly requests remaining: 5179\n",
      "   Minute requests remaining: 192\n",
      "✅ Successful enrichments: 819\n",
      "❌ Failed enrichments: 0\n",
      "📊 Success rate: 100.0%\n",
      "🎯 Optimized enrichment process completed!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from time import sleep\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "APOLLO_API_KEY = os.getenv(\"apollo_key\")\n",
    "\n",
    "def create_checkpoint_system(output_path):\n",
    "    \"\"\"Create checkpoint files for resuming interrupted runs\"\"\"\n",
    "    checkpoint_file = output_path.replace('.csv', '_checkpoint.pkl')\n",
    "    return checkpoint_file\n",
    "\n",
    "def save_checkpoint(data, checkpoint_file):\n",
    "    \"\"\"Save progress checkpoint\"\"\"\n",
    "    with open(checkpoint_file, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "def load_checkpoint(checkpoint_file):\n",
    "    \"\"\"Load progress checkpoint\"\"\"\n",
    "    try:\n",
    "        with open(checkpoint_file, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    except FileNotFoundError:\n",
    "        return {'completed_urls': set(), 'enriched_data': [], 'raw_data': []}\n",
    "\n",
    "def check_rate_limits(response_headers):\n",
    "    \"\"\"Extract and display rate limit information from response headers\"\"\"\n",
    "    rate_info = {}\n",
    "    headers_to_check = [\n",
    "        'x-daily-requests-left', 'x-daily-usage', 'x-rate-limit-daily',\n",
    "        'x-hourly-requests-left', 'x-hourly-usage', 'x-rate-limit-hourly', \n",
    "        'x-minute-requests-left', 'x-minute-usage', 'x-rate-limit-minute'\n",
    "    ]\n",
    "    \n",
    "    for header in headers_to_check:\n",
    "        if header in response_headers:\n",
    "            rate_info[header] = response_headers[header]\n",
    "    \n",
    "    # Calculate percentage used for key metrics\n",
    "    if 'x-minute-requests-left' in rate_info and 'x-rate-limit-minute' in rate_info:\n",
    "        minute_used = int(rate_info['x-rate-limit-minute']) - int(rate_info['x-minute-requests-left'])\n",
    "        minute_pct = (minute_used / int(rate_info['x-rate-limit-minute'])) * 100\n",
    "        rate_info['minute_usage_pct'] = f\"{minute_pct:.1f}%\"\n",
    "    \n",
    "    if 'x-hourly-requests-left' in rate_info and 'x-rate-limit-hourly' in rate_info:\n",
    "        hourly_used = int(rate_info['x-rate-limit-hourly']) - int(rate_info['x-hourly-requests-left'])\n",
    "        hourly_pct = (hourly_used / int(rate_info['x-rate-limit-hourly'])) * 100\n",
    "        rate_info['hourly_usage_pct'] = f\"{hourly_pct:.1f}%\"\n",
    "    \n",
    "    return rate_info\n",
    "\n",
    "def calculate_smart_delay(response_headers, base_delay=0.3):\n",
    "    \"\"\"Calculate intelligent delay based on current rate limit usage - optimized for 200/min limit\"\"\"\n",
    "    if 'x-minute-requests-left' in response_headers and 'x-rate-limit-minute' in response_headers:\n",
    "        requests_left = int(response_headers['x-minute-requests-left'])\n",
    "        rate_limit = int(response_headers['x-rate-limit-minute'])\n",
    "        \n",
    "        # With 200/min limit, we can be much more aggressive\n",
    "        if requests_left <= 10:\n",
    "            return base_delay * 10  # 3s delay when very close to limit\n",
    "        elif requests_left <= 30:\n",
    "            return base_delay * 3   # 0.9s delay when getting close\n",
    "        elif requests_left <= 50:\n",
    "            return base_delay * 2   # 0.6s delay when moderately close\n",
    "        else:\n",
    "            return base_delay       # 0.3s delay when plenty of room\n",
    "    \n",
    "    return base_delay\n",
    "\n",
    "def enrich_lead_with_linkedin_smart(linkedin_url, reveal_personal_emails=False, reveal_phone_number=False, max_retries=5):\n",
    "    \"\"\"\n",
    "    Enrich a lead using Apollo API with smart rate limiting based on response headers.\n",
    "    \"\"\"\n",
    "    url = \"https://api.apollo.io/api/v1/people/match\"\n",
    "    headers = {\n",
    "        \"accept\": \"application/json\",\n",
    "        \"Cache-Control\": \"no-cache\", \n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"x-api-key\": APOLLO_API_KEY\n",
    "    }\n",
    "    payload = {\n",
    "        \"linkedin_url\": linkedin_url,\n",
    "        \"reveal_personal_emails\": reveal_personal_emails,\n",
    "        \"reveal_phone_number\": reveal_phone_number\n",
    "    }\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.post(url, headers=headers, json=payload)\n",
    "            \n",
    "            # Extract rate limit info from headers\n",
    "            rate_info = check_rate_limits(response.headers)\n",
    "            \n",
    "            # Check for 429 (Too Many Requests)\n",
    "            if response.status_code == 429:\n",
    "                # Smart backoff based on which limit was hit\n",
    "                if 'x-minute-requests-left' in response.headers and int(response.headers['x-minute-requests-left']) == 0:\n",
    "                    wait_time = 60 + random.uniform(1, 5)  # Wait for minute window reset\n",
    "                    print(f\"\\n⚠️  Minute rate limit hit. Waiting {wait_time:.1f}s for window reset\")\n",
    "                elif 'x-hourly-requests-left' in response.headers and int(response.headers['x-hourly-requests-left']) == 0:\n",
    "                    wait_time = 3600 + random.uniform(10, 60)  # Wait for hour window reset\n",
    "                    print(f\"\\n⚠️  Hourly rate limit hit. Waiting {wait_time/60:.1f} minutes for window reset\")\n",
    "                else:\n",
    "                    # Standard exponential backoff\n",
    "                    wait_time = (2 ** attempt) + random.uniform(0, 1)\n",
    "                    print(f\"\\n⚠️  Rate limit hit (429). Waiting {wait_time:.1f}s before retry {attempt + 1}/{max_retries}\")\n",
    "                \n",
    "                sleep(wait_time)\n",
    "                continue\n",
    "            \n",
    "            # Check for other HTTP errors\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Return both data and rate info\n",
    "            result = response.json()\n",
    "            result['_rate_info'] = rate_info\n",
    "            return result\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            if attempt == max_retries - 1:  # Last attempt\n",
    "                print(f\"\\n❌ Max retries reached for {linkedin_url}: {e}\")\n",
    "                return {\"error\": str(e)}\n",
    "            else:\n",
    "                wait_time = (2 ** attempt) + random.uniform(0, 1)\n",
    "                print(f\"\\n⚠️  Request error. Waiting {wait_time:.1f}s before retry {attempt + 1}/{max_retries}: {e}\")\n",
    "                sleep(wait_time)\n",
    "    \n",
    "    return {\"error\": \"Max retries exceeded\"}\n",
    "\n",
    "def enrich_linkedin_urls_sequential_smart(df_input, linkedin_column='LinkedIn URL_cleaned', \n",
    "                                         output_csv_path='enriched_leads_sequential.csv', \n",
    "                                         base_delay=1.5, checkpoint_every=25):\n",
    "    \"\"\"\n",
    "    Sequential enrichment with smart rate limiting based on Apollo's response headers.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Setup checkpoint system\n",
    "    checkpoint_file = create_checkpoint_system(output_csv_path)\n",
    "    checkpoint_data = load_checkpoint(checkpoint_file)\n",
    "    \n",
    "    # Get all URLs to process\n",
    "    all_urls = list(df_input[linkedin_column].dropna().unique())\n",
    "    remaining_urls = [url for url in all_urls if url not in checkpoint_data['completed_urls']]\n",
    "    \n",
    "    print(f\"📊 Total URLs: {len(all_urls)}\")\n",
    "    print(f\"✅ Already completed: {len(checkpoint_data['completed_urls'])}\")\n",
    "    print(f\"⏳ Remaining to process: {len(remaining_urls)}\")\n",
    "    print(f\"🧠 Smart sequential processing with rate limit awareness\")\n",
    "    \n",
    "    if not remaining_urls:\n",
    "        print(\"🎉 All URLs already processed!\")\n",
    "        return pd.DataFrame(checkpoint_data['enriched_data']), checkpoint_data['raw_data']\n",
    "    \n",
    "    enriched_data = checkpoint_data['enriched_data']\n",
    "    raw_data_list = checkpoint_data['raw_data']\n",
    "    completed_count = len(checkpoint_data['completed_urls'])\n",
    "    last_rate_info = {}\n",
    "    \n",
    "    # Sequential processing with progress bar\n",
    "    with tqdm(total=len(remaining_urls), desc=\"Enriching leads\", initial=0) as pbar:\n",
    "        for i, linkedin_url in enumerate(remaining_urls):\n",
    "            try:\n",
    "                # Call API with smart retry logic\n",
    "                result = enrich_lead_with_linkedin_smart(linkedin_url)\n",
    "                \n",
    "                # Extract rate info if present\n",
    "                if '_rate_info' in result:\n",
    "                    last_rate_info = result.pop('_rate_info')\n",
    "                \n",
    "                flat_result, raw_result = flatten_person_data(result)\n",
    "                flat_result['original_linkedin_url'] = linkedin_url\n",
    "                \n",
    "                # Store results\n",
    "                enriched_data.append(flat_result)\n",
    "                raw_data_list.append({\n",
    "                    'original_linkedin_url': linkedin_url,\n",
    "                    'apollo_raw': raw_result\n",
    "                })\n",
    "                checkpoint_data['completed_urls'].add(linkedin_url)\n",
    "                completed_count += 1\n",
    "                \n",
    "                # Save checkpoint every N records\n",
    "                if completed_count % checkpoint_every == 0:\n",
    "                    checkpoint_data['enriched_data'] = enriched_data\n",
    "                    checkpoint_data['raw_data'] = raw_data_list\n",
    "                    save_checkpoint(checkpoint_data, checkpoint_file)\n",
    "                    print(f\"\\n💾 Checkpoint saved at {completed_count} records\")\n",
    "                \n",
    "                # Create progress info with rate limits\n",
    "                progress_info = {\n",
    "                    'Completed': completed_count,\n",
    "                    'URL': linkedin_url[:30] + '...' if len(linkedin_url) > 30 else linkedin_url,\n",
    "                    'Success': '✅' if 'error' not in flat_result else '❌'\n",
    "                }\n",
    "                \n",
    "                # Add rate limit info to progress if available\n",
    "                if 'x-minute-requests-left' in last_rate_info:\n",
    "                    progress_info['Min_Left'] = last_rate_info['x-minute-requests-left']\n",
    "                if 'x-hourly-requests-left' in last_rate_info:\n",
    "                    progress_info['Hr_Left'] = last_rate_info['x-hourly-requests-left']\n",
    "                \n",
    "                pbar.set_postfix(progress_info)\n",
    "                \n",
    "                # Show detailed rate info every 10 requests\n",
    "                if completed_count % 10 == 0 and last_rate_info:\n",
    "                    print(f\"\\n📊 Rate Limit Status:\")\n",
    "                    if 'minute_usage_pct' in last_rate_info:\n",
    "                        print(f\"   Minute: {last_rate_info['minute_usage_pct']} used\")\n",
    "                    if 'hourly_usage_pct' in last_rate_info:\n",
    "                        print(f\"   Hourly: {last_rate_info['hourly_usage_pct']} used\")\n",
    "                    if 'x-daily-requests-left' in last_rate_info:\n",
    "                        print(f\"   Daily: {last_rate_info['x-daily-requests-left']} requests left\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"\\n💥 Unexpected error processing {linkedin_url}: {e}\")\n",
    "                error_record = {'error': str(e), 'original_linkedin_url': linkedin_url}\n",
    "                enriched_data.append(error_record)\n",
    "                raw_data_list.append({'original_linkedin_url': linkedin_url, 'apollo_raw': {'error': str(e)}})\n",
    "                checkpoint_data['completed_urls'].add(linkedin_url)\n",
    "            \n",
    "            pbar.update(1)\n",
    "            \n",
    "            # Smart delay calculation based on rate limits\n",
    "            if i < len(remaining_urls) - 1:\n",
    "                smart_delay = calculate_smart_delay(last_rate_info if last_rate_info else {}, base_delay)\n",
    "                sleep(smart_delay)\n",
    "    \n",
    "    # Final save\n",
    "    df_enriched = pd.DataFrame(enriched_data)\n",
    "    \n",
    "    # Add timestamp to final files\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    final_csv = output_csv_path.replace('.csv', f'_complete_{timestamp}.csv')\n",
    "    final_json = final_csv.replace('.csv', '_raw.json')\n",
    "    \n",
    "    # Save final results\n",
    "    df_enriched.to_csv(final_csv, index=False)\n",
    "    with open(final_json, 'w') as f:\n",
    "        json.dump(raw_data_list, f, indent=2)\n",
    "    \n",
    "    # Also save latest version (overwrite)\n",
    "    df_enriched.to_csv(output_csv_path, index=False)\n",
    "    latest_json = output_csv_path.replace('.csv', '_raw.json')\n",
    "    with open(latest_json, 'w') as f:\n",
    "        json.dump(raw_data_list, f, indent=2)\n",
    "    \n",
    "    # Clean up checkpoint file\n",
    "    if os.path.exists(checkpoint_file):\n",
    "        os.remove(checkpoint_file)\n",
    "    \n",
    "    print(f\"\\n🎉 Enrichment completed!\")\n",
    "    print(f\"📁 Final results saved to: {final_csv}\")\n",
    "    print(f\"📁 Latest version: {output_csv_path}\")\n",
    "    print(f\"📁 Raw data: {final_json}\")\n",
    "    \n",
    "    # Show final rate limit status\n",
    "    if last_rate_info:\n",
    "        print(f\"\\n📊 Final Rate Limit Status:\")\n",
    "        if 'x-daily-requests-left' in last_rate_info:\n",
    "            print(f\"   Daily requests remaining: {last_rate_info['x-daily-requests-left']}\")\n",
    "        if 'x-hourly-requests-left' in last_rate_info:\n",
    "            print(f\"   Hourly requests remaining: {last_rate_info['x-hourly-requests-left']}\")\n",
    "        if 'x-minute-requests-left' in last_rate_info:\n",
    "            print(f\"   Minute requests remaining: {last_rate_info['x-minute-requests-left']}\")\n",
    "    \n",
    "    # Show summary\n",
    "    successful = df_enriched[df_enriched['error'].isna()].shape[0] if 'error' in df_enriched.columns else len(df_enriched)\n",
    "    failed = len(df_enriched) - successful\n",
    "    \n",
    "    print(f\"✅ Successful enrichments: {successful}\")\n",
    "    print(f\"❌ Failed enrichments: {failed}\")\n",
    "    print(f\"📊 Success rate: {(successful/len(df_enriched)*100):.1f}%\")\n",
    "    \n",
    "    return df_enriched, raw_data_list\n",
    "\n",
    "def flatten_person_data(apollo_result):\n",
    "    \"\"\"Flatten Apollo API result into flat dictionary for CSV export.\"\"\"\n",
    "    if 'error' in apollo_result:\n",
    "        return {'error': apollo_result['error']}, apollo_result\n",
    "    \n",
    "    person = apollo_result.get('person', {})\n",
    "    org = person.get('organization', {})\n",
    "    \n",
    "    flat_data = {\n",
    "        'apollo_id': person.get('id'),\n",
    "        'first_name': person.get('first_name'),\n",
    "        'last_name': person.get('last_name'),\n",
    "        'full_name': person.get('name'),\n",
    "        'email': person.get('email'),\n",
    "        'email_status': person.get('email_status'),\n",
    "        'linkedin_url': person.get('linkedin_url'),\n",
    "        'title': person.get('title'),\n",
    "        'headline': person.get('headline'),\n",
    "        'photo_url': person.get('photo_url'),\n",
    "        'city': person.get('city'),\n",
    "        'state': person.get('state'),\n",
    "        'country': person.get('country'),\n",
    "        'seniority': person.get('seniority'),\n",
    "        'organization_id': org.get('id'),\n",
    "        'organization_name': org.get('name'),\n",
    "        'organization_website': org.get('website_url'),\n",
    "        'organization_linkedin': org.get('linkedin_url'),\n",
    "        'organization_phone': org.get('phone'),\n",
    "        'organization_industry': org.get('industry'),\n",
    "        'organization_employees': org.get('estimated_num_employees'),\n",
    "        'organization_revenue': org.get('annual_revenue'),\n",
    "        'organization_founded': org.get('founded_year'),\n",
    "        'organization_description': org.get('short_description'),\n",
    "        'current_company': None,\n",
    "        'current_title': None,\n",
    "        'current_start_date': None,\n",
    "    }\n",
    "    \n",
    "    # Extract current employment info\n",
    "    employment_history = person.get('employment_history', [])\n",
    "    if employment_history:\n",
    "        current_job = next((job for job in employment_history if job.get('current')), employment_history[0])\n",
    "        flat_data['current_company'] = current_job.get('organization_name')\n",
    "        flat_data['current_title'] = current_job.get('title')\n",
    "        flat_data['current_start_date'] = current_job.get('start_date')\n",
    "    \n",
    "    # Extract departments and functions\n",
    "    departments = person.get('departments', [])\n",
    "    functions = person.get('functions', [])\n",
    "    flat_data['departments'] = ', '.join(departments) if departments else None\n",
    "    flat_data['functions'] = ', '.join(functions) if functions else None\n",
    "    \n",
    "    return flat_data, apollo_result\n",
    "\n",
    "# Run the OPTIMIZED sequential enrichment for your rate limits\n",
    "print(\"🚀 Starting OPTIMIZED Apollo enrichment for 200/min rate limit...\")\n",
    "print(f\"📊 Processing {len(df_final)} LinkedIn URLs\")\n",
    "\n",
    "df_enriched_full, raw_data_full = enrich_linkedin_urls_sequential_smart(\n",
    "    df_input=df_final,\n",
    "    linkedin_column='LinkedIn URL_cleaned',\n",
    "    output_csv_path='enriched_leads_abu_dhabi_commercial_bank_optimized.csv',\n",
    "    base_delay=0.3,  # Much faster: 0.3s delay = ~200 requests/min (perfect for your limit!)\n",
    "    checkpoint_every=50  # Save checkpoint every 50 records\n",
    ")\n",
    "\n",
    "print(\"🎯 Optimized enrichment process completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ff888aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Current Apollo API Rate Limits:\n",
      "==================================================\n",
      "x-hourly-requests-left: 5998\n",
      "x-rate-limit-hourly: 6000\n",
      "x-minute-requests-left: 199\n",
      "x-rate-limit-minute: 200\n",
      "\n",
      "📊 Minute Usage: 1/200 (0.5%)\n",
      "📊 Hourly Usage: 2/6000 (0.0%)\n",
      "\n",
      "📡 Response Status: 200\n"
     ]
    }
   ],
   "source": [
    "# Quick rate limit check\n",
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "APOLLO_API_KEY = os.getenv(\"apollo_key\")\n",
    "\n",
    "def quick_rate_limit_check():\n",
    "    \"\"\"Quick check of current Apollo API rate limits\"\"\"\n",
    "    url = \"https://api.apollo.io/api/v1/people/match\"\n",
    "    headers = {\n",
    "        \"accept\": \"application/json\",\n",
    "        \"Cache-Control\": \"no-cache\",\n",
    "        \"Content-Type\": \"application/json\", \n",
    "        \"x-api-key\": APOLLO_API_KEY\n",
    "    }\n",
    "    \n",
    "    # Use a dummy/test LinkedIn URL for the check\n",
    "    payload = {\n",
    "        \"linkedin_url\": \"https://www.linkedin.com/in/test-profile-check\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=payload)\n",
    "        \n",
    "        print(\"🔍 Current Apollo API Rate Limits:\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Extract rate limit headers\n",
    "        rate_headers = [\n",
    "            'x-daily-requests-left', 'x-rate-limit-daily',\n",
    "            'x-hourly-requests-left', 'x-rate-limit-hourly',\n",
    "            'x-minute-requests-left', 'x-rate-limit-minute'\n",
    "        ]\n",
    "        \n",
    "        for header in rate_headers:\n",
    "            if header in response.headers:\n",
    "                print(f\"{header}: {response.headers[header]}\")\n",
    "        \n",
    "        # Calculate usage percentages\n",
    "        if 'x-minute-requests-left' in response.headers and 'x-rate-limit-minute' in response.headers:\n",
    "            left = int(response.headers['x-minute-requests-left'])\n",
    "            limit = int(response.headers['x-rate-limit-minute'])\n",
    "            used = limit - left\n",
    "            usage_pct = (used / limit) * 100\n",
    "            print(f\"\\n📊 Minute Usage: {used}/{limit} ({usage_pct:.1f}%)\")\n",
    "            \n",
    "        if 'x-hourly-requests-left' in response.headers and 'x-rate-limit-hourly' in response.headers:\n",
    "            left = int(response.headers['x-hourly-requests-left'])\n",
    "            limit = int(response.headers['x-rate-limit-hourly'])\n",
    "            used = limit - left  \n",
    "            usage_pct = (used / limit) * 100\n",
    "            print(f\"📊 Hourly Usage: {used}/{limit} ({usage_pct:.1f}%)\")\n",
    "            \n",
    "        if 'x-daily-requests-left' in response.headers and 'x-rate-limit-daily' in response.headers:\n",
    "            left = int(response.headers['x-daily-requests-left'])\n",
    "            limit = int(response.headers['x-rate-limit-daily'])\n",
    "            used = limit - left\n",
    "            usage_pct = (used / limit) * 100\n",
    "            print(f\"📊 Daily Usage: {used}/{limit} ({usage_pct:.1f}%)\")\n",
    "        \n",
    "        print(f\"\\n📡 Response Status: {response.status_code}\")\n",
    "        \n",
    "        return response.headers\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error checking rate limits: {e}\")\n",
    "        return None\n",
    "\n",
    "# Run the check\n",
    "headers = quick_rate_limit_check()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
